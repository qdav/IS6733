{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.preprocessing\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import combinations\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics as skm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    #format=\"%(asctime)s %(message)s\",\n",
    "    format=\"%(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"{0}.log\".format('pitch_prediction')),\n",
    "        logging.StreamHandler()\n",
    "    ])\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "pd.options.display.width = 0\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# decode labels for outcomes and pitch types\n",
    "\n",
    "# outcome codes\n",
    "outcome_code = {\n",
    "    'outcome_code' : ['B', '*B', 'S', 'C',\n",
    "                      'F', 'T', 'L', 'I',\n",
    "                      'W', 'M', 'P',\n",
    "                      'Q','R', 'X',\n",
    "                      'D', 'E', 'H',\n",
    "                      'V', 'Z'],\n",
    "    'outcome_description' : ['Ball', 'Ball in Dirt', 'Swinging Strike', 'Called Strike',\n",
    "                            'Foul', 'Foul Tip', 'Foul Bunt', 'Intentional Ball',\n",
    "                            'Swinging Strike (Blocked)', 'Missed Bunt', 'Pitchout',\n",
    "                            'Swinging Pitchout', 'Foul Pitchout', 'In Play, Out(s)',\n",
    "                            'In Play, No Outs', 'In Play, Runs', 'Hit by pitch',\n",
    "                            'V', 'Z']\n",
    "}\n",
    "outcome_code = pd.DataFrame.from_dict(outcome_code)\n",
    "\n",
    "pitch_code = {\n",
    "    'pitch_code': ['CH', 'CU', 'EP', 'FC',\n",
    "                   'FF', 'FO', 'FS', 'FT',\n",
    "                   'IN', 'KC', 'KN',\n",
    "                   'PO', 'SC', 'SI',\n",
    "                   'SL', 'UN', 'FA', 'AB'],\n",
    "    'pitch_description': ['Changeup', 'Curveball', 'Eephus', 'Cutter',\n",
    "                          'Four-seam Fastball', 'Pitchout', 'Splitter', 'Two-seam Fastball',\n",
    "                          'Intentionall ball', 'Knuckle curve', 'Knuckleball',\n",
    "                          'Pitchout', 'Screwball', 'Sinker',\n",
    "                          'Slider', 'Unknown', 'FA', 'AB']\n",
    "}\n",
    "pitch_code = pd.DataFrame.from_dict(pitch_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def load_data(num_rec=0):\n",
    "    # specify subset of records to use\n",
    "    if num_rec > 0:\n",
    "        pitches = pd.read_csv(\"data/pitches.csv\", nrows=num_rec)\n",
    "    else:\n",
    "        pitches = pd.read_csv(\"data/pitches.csv\")\n",
    "    atbats = pd.read_csv(\"data/atbats.csv\")\n",
    "    ejections = pd.read_csv(\"data/ejections.csv\")\n",
    "    games = pd.read_csv(\"data/games.csv\")\n",
    "    player_names = pd.read_csv(\"data/player_names.csv\")\n",
    "\n",
    "\n",
    "\n",
    "    # join all the data into all_df\n",
    "    all_df = pd.merge(pitches, atbats, how='left',\n",
    "            left_on = 'ab_id', right_on = 'ab_id')\n",
    "\n",
    "    pitcher_df = player_names\n",
    "    pitcher_df.columns = ['pitcher_id', 'pitcher_first_name', 'pitcher_last_name']\n",
    "\n",
    "    all_df = pd.merge(all_df, pitcher_df, how='left',\n",
    "            left_on = 'pitcher_id', right_on = 'pitcher_id')\n",
    "\n",
    "    batter_df = player_names\n",
    "    batter_df.columns = ['batter_id', 'batter_first_name', 'batter_last_name']\n",
    "\n",
    "    all_df = pd.merge(all_df, batter_df, how='left',\n",
    "            left_on = 'batter_id', right_on = 'batter_id')\n",
    "\n",
    "    all_df = pd.merge(all_df, games, how='left',\n",
    "            left_on = 'g_id', right_on = 'g_id')\n",
    "\n",
    "    all_df = pd.merge(all_df, pitch_code, how='left',\n",
    "            left_on = 'pitch_type', right_on = 'pitch_code')\n",
    "\n",
    "    all_df = pd.merge(all_df, outcome_code, how='left',\n",
    "            left_on = 'code', right_on = 'outcome_code')\n",
    "\n",
    "    # extract weather conditions\n",
    "    all_df['temp'] = all_df['weather'].str.extract(r'(\\d+)')  # temperature\n",
    "    all_df['temp'] = pd.to_numeric(all_df['temp'])\n",
    "    all_df['weather_cond'] = all_df['weather'].str.extract(r'([^,]*$)')  # weather condition\n",
    "    all_df['wind_mph'] = all_df['wind'].str.extract(r'(\\d+)')  # wind speed\n",
    "    all_df['wind_mph'] = pd.to_numeric(all_df['wind_mph'])\n",
    "    all_df['wind_dir'] = all_df['wind'].str.extract(r'([^,]*$)')  # wind direction\n",
    "\n",
    "    all_df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "    return all_df\n",
    "\n",
    "def encode_data(df_in, x_col_list, y_col, avail_cols, y_encode='none', factor=np.nan, x_encode=True):\n",
    "    col_no_encode = []\n",
    "    # we know there's one dependent variable to seed a dataframe\n",
    "    y_df = df_in[[y_col]]\n",
    "    encode_df = y_df\n",
    "\n",
    "    for col in x_col_list:\n",
    "        if avail_cols.get(col):\n",
    "            # one-hot encode\n",
    "            dummies_df = pd.get_dummies(df_in[col], prefix=col)\n",
    "            encode_df = pd.concat([encode_df, dummies_df], axis=1)\n",
    "        else:\n",
    "            # just retrieve the column directly\n",
    "            col_no_encode.append(col)\n",
    "\n",
    "    encode_df = encode_df.drop([y_col], axis=1) # remove dependent variable seeding column\n",
    "    if x_encode:\n",
    "        X = pd.concat([encode_df, df_in[col_no_encode]], axis=1)\n",
    "    else:\n",
    "        X = df_in[x_col_list]\n",
    "\n",
    "    if y_encode=='factorize':\n",
    "        # use factorized y value\n",
    "        y_return = factor[0]\n",
    "    elif y_encode=='onehot':\n",
    "        # one hot encode y values\n",
    "        y_return = pd.get_dummies(y_df, prefix=y_col)\n",
    "    else:\n",
    "        y_return = y_df\n",
    "\n",
    "    y = y_return\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# functions to evaluate the results of different models and write to file\n",
    "\n",
    "def format_cm(y_test_in, y_pred_in, factor_in):\n",
    "    # confusion matrix\n",
    "    y_test_label = factor_in[1][y_test_in]\n",
    "    y_pred_label = factor_in[1][y_pred_in]\n",
    "    cm = pd.DataFrame(\n",
    "        confusion_matrix(y_test_label, y_pred_label, labels=list(factor_in[1])),\n",
    "        index=list(factor_in[1]),\n",
    "        columns=list(factor_in[1])\n",
    "    )\n",
    "    return cm\n",
    "\n",
    "\n",
    "def eval_rf(y_test_in, y_pred_in, classifier_in, factor_in, idx_in, vars_in, elapsed_time, col_labels):\n",
    "    logger.info(f'\\nRandom Forest Test ' + str(idx_in))\n",
    "    logger.info(str(vars_in))\n",
    "\n",
    "    # confusion matrix\n",
    "    cm = format_cm(y_test_in, y_pred_in, factor_in)\n",
    "    logger.info(cm)\n",
    "\n",
    "    # print variable importance\n",
    "    #df_x_train = pd.DataFrame(X_train)\n",
    "\n",
    "    feature_importances = pd.DataFrame(classifier_in.feature_importances_,\n",
    "                                       index = col_labels,\n",
    "                                        columns=['importance']).sort_values('importance', ascending=False)\n",
    "    #train_col_df = pd.DataFrame(df_x_train.columns)\n",
    "    #fi = pd.merge(feature_importances, col_labels_df, how='left',\n",
    "    #        left_index = True, right_index = True)\n",
    "\n",
    "    #logger.info('\\n' + str(feature_importances.iloc[:20]))\n",
    "    logger.info('\\n' + str(feature_importances))\n",
    "\n",
    "    pscore = metrics.accuracy_score(y_test_in, y_pred_in)\n",
    "    logger.info(f'Random Forest accuracy {pscore}')\n",
    "    logger.info('Model time: %.1f [sec]' % (elapsed_time))\n",
    "\n",
    "def eval_knn(y_test_in, y_pred_in, factor_in, idx_in, vars_in, elapsed_time):\n",
    "    logger.info(f'\\nKNN Test ' + str(idx_in))\n",
    "    logger.info(str(vars_in))\n",
    "\n",
    "    # confusion matrix\n",
    "    cm = format_cm(y_test_in, y_pred_in, factor_in)\n",
    "    logger.info(cm)\n",
    "\n",
    "    # accuracy/time\n",
    "    pscore = metrics.accuracy_score(y_test_in, y_pred_in)\n",
    "    logger.info(f'KNN accuracy {pscore}')\n",
    "    logger.info('Model time: %.1f [sec]' % (elapsed_time))\n",
    "\n",
    "def eval_svc(y_test_in, y_pred_in, factor_in, idx_in, vars_in, elapsed_time):\n",
    "    logger.info(f'\\nSVC Test ' + str(idx_in))\n",
    "    logger.info(str(vars_in))\n",
    "\n",
    "    # confusion matrix\n",
    "    cm = format_cm(y_test_in, y_pred_in, factor_in)\n",
    "    logger.info(cm)\n",
    "\n",
    "    # accuracy/time\n",
    "    pscore = metrics.accuracy_score(y_test_in, y_pred_in)\n",
    "    logger.info(f'SVC accuracy {pscore}')\n",
    "    logger.info('Model time: %.1f [sec]' % (elapsed_time))\n",
    "\n",
    "def eval_nn(y_test_in, y_pred_in, history_in, factor_in, idx_in, vars_in, elapsed_time_in):\n",
    "    logger.info(f'\\nNeural Net Test ' + str(idx_in))\n",
    "    logger.info(str(vars_in))\n",
    "\n",
    "    # model architecture\n",
    "    print(str(history_in.model.summary()))\n",
    "    logger.info(str(history_in.model.summary()))\n",
    "\n",
    "    # accuracy/loss by epoch\n",
    "    hist = pd.DataFrame(history_in.history)\n",
    "    hist['epoch'] = history_in.epoch\n",
    "    logger.info('\\n' + str(hist))\n",
    "\n",
    "    # plot\n",
    "    acc = history_in.history['accuracy']\n",
    "    val_acc = history_in.history['val_accuracy']\n",
    "    loss = history_in.history['loss']\n",
    "    val_loss = history_in.history['val_loss']\n",
    "\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # exit accuracy and loss\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "    logger.info(f'\\nExit test accuracy: {test_acc}')\n",
    "    logger.info(f'Exit test loss: {test_loss}')\n",
    "\n",
    "    # confusion matrix\n",
    "    cm = format_cm(y_test_in, y_pred_in, factor_in)\n",
    "    logger.info(cm)\n",
    "\n",
    "    # accuracy/time\n",
    "    pscore = metrics.accuracy_score(y_test_in, y_pred_in)\n",
    "    logger.info(f'Neural Net accuracy {pscore}')\n",
    "    logger.info('Model time: %.1f [sec]' % (elapsed_time_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# These are the values one can pass in as variables\n",
    "# Values specify whether the column should be one-hot encoded\n",
    "# Some are known at the time of pitch and some are outcomes of a pitch\n",
    "avail_cols = {\n",
    "    'px': False,        'pz': False,        'start_speed': False,   'end_speed': False,\n",
    "    'spin_rate': False, 'spin_dir': False,  'break_angle': False,   'break_length': False,\n",
    "    'break_y': False,   'ax': False,        'ay': False,            'az': False,\n",
    "    'sz_bot': False,    'sz_top': False,    'type_confidence': True,'vx0': False,\n",
    "    'vy0': False,       'vz0': False,       'x': False,             'x0': False,\n",
    "    'y': False,         'y0': False,        'z0': False,            'pfx_x': False,\n",
    "    'pfx_z': False,     'nasty': False,     'zone': True,           'code':\tTrue,\n",
    "    'type': True,       'pitch_type': True, 'event_num': False,     'b_score': False,\n",
    "    'ab_id' : True,     'b_count': True,    's_count': True,        'outs': True,\n",
    "    'pitch_num': False, 'on_1b': False,     'on_2b': False,         'on_3b': False,\n",
    "    'batter_id': True,  'event': True,      'g_id': True,           'inning': True,     'o': True,\n",
    "    'p_score': False,   'p_throws': True,   'pitcher_id': True,     'stand': True,      'top': False,\n",
    "    'pitcher_first_name': True,     'pitcher_last_name': True,      'batter_first_name': True,\n",
    "    'batter_last_name': True,       'attendance': False,            'away_final_score': False,\n",
    "    'away_team': True,  'date': True,       'elapsed_time': False,  'home_final_score': False,\n",
    "    'home_team': True,  'start_time': False, 'umpire_1B': True,      'umpire_2B': True,\n",
    "    'umpire_3B': True,  'umpire_HP': True,  'venue_name': True,     'weather': True,\n",
    "    'wind': True,       'delay': False,     'pitch_code': True,     'pitch_description': True,\n",
    "    'outcome_code': True,           'outcome_description': True,    'temp': False,\n",
    "    'weather_cond': True,           'wind_mph': False,              'wind_dir': True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data from the files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# load data, calculate new columns, join data - optional number of records parameter\n",
    "all_df = load_data(10000) # pick the first n records if desired\n",
    "#all_df = load_data()\n",
    "#all_df = all_df[(all_df['date'] >= '2018-01-01') & (all_df['date'] <= '2018-12-31')]  # 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data for baseline statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# pitch outcomes\n",
    "outcome_df = all_df.groupby(['outcome_code', 'outcome_description']).size().reset_index(name='outcome_count')\n",
    "outcome_df['outcome_prob']= outcome_df['outcome_count']/outcome_df['outcome_count'].sum()\n",
    "outcome_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# bar chart of pitch outcomes\n",
    "objects = outcome_df.outcome_description\n",
    "y_pos = np.arange(len(outcome_df.outcome_description))\n",
    "performance = outcome_df.outcome_count\n",
    "\n",
    "plt.barh(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.yticks(y_pos, objects)\n",
    "plt.xlabel('Pitches')\n",
    "plt.title('MLB 2015-2018')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# pitch outcomes\n",
    "pitch_df = all_df.groupby(['pitch_code', 'pitch_description']).size().reset_index(name='pitch_type_count')\n",
    "\n",
    "pitch_df['pitch_prob']= pitch_df['pitch_type_count']/pitch_df['pitch_type_count'].sum()\n",
    "pitch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# bar chart of pitch types\n",
    "objects = pitch_df.pitch_description\n",
    "y_pos = np.arange(len(pitch_df.pitch_description))\n",
    "performance = pitch_df.pitch_type_count\n",
    "\n",
    "plt.barh(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.yticks(y_pos, objects)\n",
    "plt.xlabel('Pitches')\n",
    "plt.title('MLB 2015-2018')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# create one list of variables for each test you want to run\n",
    "var_list = []\n",
    "\n",
    "# this list has all the predictors known before a pitch takes place\n",
    "# var_list.append(['b_score', 'b_count', 's_count','pitch_num', 'on_1b', 'on_2b', 'on_3b', 'batter_id', 'inning', 'o',\n",
    "#                  'p_score', 'p_throws', 'stand', 'top', 'batter_id', 'pitcher_id',\n",
    "#                  'away_team', 'home_team', 'umpire_HP','venue_name', 'temp', 'weather_cond',\n",
    "#                  'wind_mph', 'wind_dir', 'delay'])\n",
    "\n",
    "# this section for passing in fixed lists of predictors\n",
    "# This list has the best prediction so far\n",
    "var_list.append(['top', 'on_1b', 'on_2b', 'on_3b', 'o', 'p_score', 'stand', 'delay', 'b_count', 's_count',\n",
    "               'pitcher_id', 'pitch_num'])\n",
    "# var_list.append(['pitch_num', 'top', 'on_1b', 'on_2b', 'on_3b', 'o', 'p_score', 'stand', 'delay', 'b_count', 's_count', 'pitcher_id'])\n",
    "fixed_vars = []\n",
    "\n",
    "# this section for mixing a fixed number of predictors with a set number of predictors\n",
    "# put the list of fixed predictors in the fixed_vars list\n",
    "# put the mutually exclusive list of candidate predictors is the var_options list\n",
    "\n",
    "# var_options = []\n",
    "# fixed_vars  =['top', 'on_1b', 'on_2b', 'on_3b', 'o', 'p_score', 'stand', 'delay', 'b_count', 's_count',\n",
    "#               'pitcher_id', 'pitch_num']\n",
    "# var_options.append(['b_score', 'batter_id', 'inning', 'p_throws',\n",
    "#                  'away_team', 'home_team', 'umpire_HP','venue_name', 'temp',\n",
    "#                  'weather_cond', 'wind_mph', 'wind_dir'])\n",
    "# var_list = list(combinations(var_options[0], 1))\n",
    "# random.shuffle(var_list)\n",
    "\n",
    "\n",
    "# factorize dependent variable\n",
    "factor = pd.factorize(all_df['pitch_code'])\n",
    "pitch_code_def = factor[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Model (fails to converge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# loop through different combinations of predictors and run linear model\n",
    "# NOTE: the linear model doesn't converge\n",
    "# for idx, vars in enumerate(var_list):\n",
    "#     var1 = list(vars)\n",
    "#     var1.extend(fixed_vars)\n",
    "#     vars = var1\n",
    "#\n",
    "#     # one-hot encode all necessary data and split into test/training data\n",
    "#     X, y = encode_data(all_df, x_col_list=vars, y_col='pitch_code', avail_cols=avail_cols,\n",
    "#                        y_encode='factorize', factor=factor)\n",
    "#\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, shuffle=False)\n",
    "#\n",
    "#     X_train = sklearn.preprocessing.scale(X_train, copy=False)\n",
    "#     X_test = sklearn.preprocessing.scale(X_test, copy=False)\n",
    "#\n",
    "#     mul_lr = linear_model.LogisticRegression(multi_class='multinomial', solver='saga').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start of Random Forest calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#loop through different combinations of predictors and run random forest calculations\n",
    "for idx, vars in enumerate(var_list):\n",
    "\n",
    "    # one-hot encode all necessary data and split into test/training data\n",
    "    X, y = encode_data(all_df, x_col_list=vars, y_col='pitch_code', avail_cols=avail_cols,\n",
    "                       y_encode='factorize', factor=factor)\n",
    "    #X = sklearn.preprocessing.scale(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, shuffle=False)\n",
    "    X_train = sklearn.preprocessing.scale(X_train, copy=False)\n",
    "    X_test = sklearn.preprocessing.scale(X_test, copy=False)\n",
    "\n",
    "    t1_start = time.perf_counter()\n",
    "\n",
    "    # create and train random forest model\n",
    "    classifier = RandomForestClassifier(n_estimators = 50, criterion = 'entropy', random_state = 42)\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # predict test data\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    t1_stop = time.perf_counter()\n",
    "    elapsed_time = t1_stop - t1_start\n",
    "\n",
    "    # print metrics for random forest model\n",
    "    eval_rf(y_test, y_pred, classifier, factor, idx, vars, elapsed_time, X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#loop through different combinations of predictors and run k-nearest neighbor\n",
    "for idx, vars in enumerate(var_list):\n",
    "    # add the fixed vars to the random ones\n",
    "    var1 = list(vars)\n",
    "    var1.extend(fixed_vars)\n",
    "    vars = var1\n",
    "\n",
    "    # one-hot encode all necessary data and split into test/training data\n",
    "    X, y = encode_data(all_df, x_col_list=vars, y_col='pitch_code', avail_cols=avail_cols,\n",
    "                       y_encode='factorize', factor=factor)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, shuffle=False)\n",
    "\n",
    "    X_train = sklearn.preprocessing.scale(X_train, copy=False)\n",
    "    X_test = sklearn.preprocessing.scale(X_test, copy=False)\n",
    "\n",
    "    t1_start = time.perf_counter()\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = knn.predict(X_test)\n",
    "\n",
    "    t1_stop = time.perf_counter()\n",
    "    elapsed_time = t1_stop - t1_start\n",
    "    eval_knn(y_test, y_pred, factor, idx, vars, elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#loop through different combinations of predictors and run SVM calculations\n",
    "for idx, vars in enumerate(var_list):\n",
    "\n",
    "    # one-hot encode all necessary data and split into test/training data\n",
    "    X, y = encode_data(all_df, x_col_list=vars, y_col='pitch_code', avail_cols=avail_cols,\n",
    "                       y_encode='factorize', factor=factor)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, shuffle=False)\n",
    "    X_train = sklearn.preprocessing.scale(X_train, copy=False)\n",
    "    X_test = sklearn.preprocessing.scale(X_test, copy=False)\n",
    "\n",
    "    # tuned the SVC model on a small (10K) number of records using grid search\n",
    "    #params_grid = [{'kernel': ['rbf'], 'gamma': [0.1, 1, 10, 100], 'C': [0.1, 1, 10, 100, 1000]},\n",
    "    #               {'kernel': ['poly'], 'degree': [1,2,3,4,5,6], 'C': [0.1, 1, 10, 100, 1000]}\n",
    "    #               # ,{'kernel': ['linear'], 'C': [1, 10, 100, 1000]} # linear classifier not suitable for this data\n",
    "    #               ]\n",
    "    #svm_model = GridSearchCV(SVC(gamma='auto'), params_grid, cv=2)\n",
    "    #print('Best score for training data:', svm_model.best_score_, \"\\n\")\n",
    "    #print('Best C:', svm_model.best_estimator_.C, \"\\n\")\n",
    "    #print('Best Kernel:', svm_model.best_estimator_.kernel, \"\\n\")\n",
    "    #print('Best Gamma:', svm_model.best_estimator_.gamma, \"\\n\")\n",
    "    #final_model = svm_model.best_estimator_\n",
    "\n",
    "    t1_start = time.perf_counter()\n",
    "\n",
    "    # used grid search to find best parameters\n",
    "    final_model= SVC(C=1, kernel='poly', degree=1)\n",
    "    final_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = final_model.predict(X_test)\n",
    "\n",
    "    t1_stop = time.perf_counter()\n",
    "    elapsed_time = t1_stop - t1_start\n",
    "\n",
    "    eval_svc(y_test, y_pred, factor, idx, vars, elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start of Multi-layer network (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# loop through different combinations of predictors and run neural network calculations\n",
    "for idx, vars in enumerate(var_list):\n",
    "    # add the fixed vars to the random ones\n",
    "    var1 = list(vars)\n",
    "    var1.extend(fixed_vars)\n",
    "    vars = var1\n",
    "\n",
    "    # one-hot encode all necessary data and split into test/training data\n",
    "    X, y = encode_data(all_df, x_col_list=vars, y_col='pitch_code', avail_cols=avail_cols,\n",
    "                       y_encode='onehot', factor=factor)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, shuffle=False)\n",
    "\n",
    "    X_train = sklearn.preprocessing.scale(X_train, copy=False)\n",
    "    X_test = sklearn.preprocessing.scale(X_test, copy=False)\n",
    "\n",
    "    t1_start = time.perf_counter()\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_dim=np.size(X_train,1), activation='relu'))\n",
    "    model.add(Dense(256, input_dim=np.size(X_train,1), activation='relu'))\n",
    "    model.add(Dense(np.size(y_train,1), activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_train,\n",
    "                        y_train,\n",
    "                        epochs=10,\n",
    "                        batch_size=512,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        verbose=True)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    #matrix = metrics.confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "\n",
    "    t1_stop = time.perf_counter()\n",
    "    elapsed_time = t1_stop - t1_start\n",
    "\n",
    "    factor = pd.factorize([x.split('pitch_code_')[1] for x in y_test.columns])\n",
    "    eval_nn(np.array(y_test).argmax(axis=1), y_pred.argmax(axis=1), history, factor, idx, vars, elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# experimenting with FastAI\n",
    "from fastai.tabular import *\n",
    "\n",
    "for idx, vars in enumerate(var_list):\n",
    "    # have to add the dependent variable back in\n",
    "    fastai_var_list = vars\n",
    "    fastai_var_list.append('pitch_code')\n",
    "    df = all_df[fastai_var_list]\n",
    "    procs = [FillMissing, Categorify, Normalize]\n",
    "    dep_var = 'pitch_code'\n",
    "    len = len(df)\n",
    "    valid_idx = range(len-(int(len*.2)), len)\n",
    "    cat_names = ['o', 'stand', 'b_count', 's_count', 'pitcher_id']\n",
    "    procs = [FillMissing, Categorify, Normalize]\n",
    "\n",
    "    data = TabularDataBunch.from_df('.', df, dep_var, valid_idx=valid_idx, procs=procs, cat_names=cat_names)\n",
    "    print(data.train_ds.cont_names)  # `cont_names` defaults to: set(df)-set(cat_names)-{dep_var}\n",
    "\n",
    "    logger.info('FastAI Model')\n",
    "    t1_start = time.perf_counter()\n",
    "\n",
    "    learn = tabular_learner(data, layers=[512,256], emb_szs={'native-country': 10}, metrics=accuracy)\n",
    "    learn.fit_one_cycle(8, 1e-2)\n",
    "    t1_stop = time.perf_counter()\n",
    "    elapsed_time = t1_stop - t1_start\n",
    "\n",
    "    logger.info('Model time: %.1f [sec]' % (elapsed_time))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
